{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn. feature_extraction. text import TfidfTransformer , TfidfVectorizer , CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "from nltk. corpus import stopwords\n",
    "from nltk. stem import PorterStemmer\n",
    "from nltk. stem. wordnet import WordNetLemmatizer\n",
    "from gensim. models import Word2Vec, KeyedVectors\n",
    "import pickle\n",
    "import scipy as sp\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn. cross_validation import train_test_split\n",
    "from sklearn. neighbors import KNeighborsClassifier\n",
    "from sklearn. cross_validation import cross_val_score\n",
    "from collections import Counter\n",
    "from sklearn import cross_validation\n",
    "from sklearn. preprocessing import normalize, StandardScaler\n",
    "from sklearn import datasets, neighbors\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "path = \"C:/Python/Assignments/Preprocessing/\"\n",
    "\n",
    "def opn(name):\n",
    "    with open(path + name , \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "with open(path +\"X_test.txt\", \"rb\") as file:\n",
    "    X_test = pickle.load(file)\n",
    "with open(path +\"X_train.txt\", \"rb\") as file:\n",
    "    X_train = pickle.load(file)\n",
    "with open(path +\"X_cv.txt\", \"rb\") as file:\n",
    "    X_cv = pickle.load(file)\n",
    "    \n",
    "with open(path +\"Y_test.txt\", \"rb\") as file:\n",
    "    Y_test = pickle.load(file)\n",
    "with open(path +\"Y_train.txt\", \"rb\") as file:\n",
    "    Y_train = pickle.load(file)\n",
    "with open(path +\"Y_cv.txt\", \"rb\") as file:\n",
    "    Y_cv = pickle.load(file)\n",
    "\n",
    "with open(path + \"X_cv_sum.txt\", \"rb\") as file:\n",
    "    X_cv_sum = pickle.load(file)\n",
    "with open(path + \"X_train_sum.txt\", \"rb\") as file:\n",
    "    X_train_sum = pickle.load(file)\n",
    "with open(path + \"X_test_sum.txt\", \"rb\") as file:\n",
    "    X_test_sum = pickle.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF function using X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "def DT(train1,test1,cv1):\n",
    "\n",
    "    sc = StandardScaler(with_mean=False)\n",
    "    train = sc.fit_transform(train1)\n",
    "    test = sc.transform(test1)\n",
    "    \n",
    "    clf = xgb.XGBClassifier(max_depth=8,n_estimators=500)\n",
    "    clf.fit(train,Y_train)\n",
    "    pred_prob = clf.predict_proba(test)\n",
    "    print(\"The AUC value for test data is \",roc_auc_score(Y_test, pred_prob[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Helpfulness percentage to add as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the helpfulness data\n",
    "with open(path + \"final_sorted.txt\", \"rb\") as file:\n",
    "    sorted_data = pickle.load(file)\n",
    "my_final = sorted_data[:100000]\n",
    "my_final['Helpful_percent'] = (my_final['HelpfulnessNumerator'] / my_final['HelpfulnessDenominator'])*100\n",
    "my_final['Helpful_percent'].fillna(0, inplace=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = my_final['Helpful_percent'].values\n",
    "y = my_final['Score']\n",
    "\n",
    "# split the data set into train and test\n",
    "X_1, X_test_perc, y_1, Y_test1 = cross_validation.train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# split the train data set into cross validation train and cross validation test\n",
    "X_train_perc, X_cv_perc, Y_train1, Y_cv1 = cross_validation.train_test_split(X_1, y_1, test_size=0.3,random_state=0)\n",
    "\n",
    "# https://stackoverflow.com/questions/41927781/adding-pandas-columns-to-a-sparse-matrix/41948136\n",
    "from scipy.sparse import hstack\n",
    "def addmat(col,csr_mat):\n",
    "    '''\n",
    "    Add a new column to sparse matrix with the number of words in each entry of col\n",
    "    '''\n",
    "    leng = []\n",
    "    for i in col:\n",
    "        leng.append(len(i.split(\" \")))\n",
    "    temp = hstack((csr_mat, pd.Series(leng)[:,None]))\n",
    "    return temp.tocsr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With review text and summary both\n",
      "\n",
      "\n",
      "The AUC value for test data is  0.9419054106873062\n",
      "With review text length\n",
      "\n",
      "\n",
      "The AUC value for test data is  0.94151889638981\n",
      "\n",
      "\n",
      "With review summary length\n",
      "\n",
      "\n",
      "The AUC value for test data is  0.9423949163768021\n",
      "\n",
      "\n",
      "With helpfullness percentage\n",
      "\n",
      "\n",
      "The AUC value for test data is  0.9560893887162235\n"
     ]
    }
   ],
   "source": [
    "#Adding review text and review summary to create a new data\n",
    "train = X_train + X_train_sum\n",
    "cv = X_cv + X_cv_sum\n",
    "test = X_test + X_test_sum\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "train_vect = tf_idf_vect.fit_transform(train)\n",
    "cv_vect = tf_idf_vect.transform(cv)\n",
    "ts_vect = tf_idf_vect.transform(test)\n",
    "print(\"With review text and summary both\\n\\n\")\n",
    "\n",
    "DT(train_vect,ts_vect,cv_vect)\n",
    "print(\"With review text length\\n\\n\")\n",
    "# Adding the number of words each review text as a new feature\n",
    "tr1 = addmat(X_train, train_vect)\n",
    "cv1 = addmat(X_cv, cv_vect)\n",
    "ts1 = addmat(X_test, ts_vect)\n",
    "DT(tr1,ts1,cv1)\n",
    "print(\"\\n\\nWith review summary length\\n\\n\")\n",
    "# Adding the number of words each review summary as a new feature\n",
    "tr1 = addmat(X_train_sum, tr1)\n",
    "cv1 = addmat(X_cv_sum, cv1)\n",
    "ts1 = addmat(X_test_sum, ts1)\n",
    "\n",
    "DT(tr1,ts1,cv1)\n",
    "print(\"\\n\\nWith helpfullness percentage\\n\\n\")\n",
    "# Adding Helpfulness Percentage as a new feature\n",
    "tr1 = hstack((tr1, pd.Series(X_train_perc)[:,None]))\n",
    "cv1 = hstack((cv1, pd.Series(X_cv_perc)[:,None]))\n",
    "ts1 = hstack((ts1, pd.Series(X_test_perc)[:,None]))\n",
    "DT(tr1,ts1,cv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------------+------+\n",
      "| Vectorizer |          New Feature           | AUC  |\n",
      "+------------+--------------------------------+------+\n",
      "|   TFIDF    |        Only Review Text        | 0.94 |\n",
      "|   TFIDF    | Review Summary along with Text | 0.94 |\n",
      "|   TFIDF    |       Review text length       | 0.94 |\n",
      "|   TFIDF    |     Review summary length      | 0.94 |\n",
      "|   TFIDF    |     Helpfulness Percentage     | 0.96 |\n",
      "+------------+--------------------------------+------+\n"
     ]
    }
   ],
   "source": [
    "# Please compare all your models using Prettytable library\n",
    "from prettytable import from_csv\n",
    "with open(\"added.csv\", \"r\") as fp: \n",
    "    x = from_csv(fp)\n",
    "\n",
    "print(x)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I have used XGBOOST with tfidf as this gave me the best AUC score before, So I am trying to improve upon that.\n",
    "2. There is improvement on the model after adding the helpful percentage as a new feature. The AUC score got bumped upto 96% from 94%.\n",
    "3. There may be other features which will improve the model more than this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
